\chapter{Methodology}
\section{Related Work}
Now first of all there already has been a decent amount of approaches for automatic text-summarization.\\
One of the oldest and most cited papers from 2002 belongs to ``Automatic Text Summarization Using a Machine Learning Approach'' from \cite{10.1007/3-540-36127-8_20}. It describes a summarization procedure based on naive Bayes and C4.5 decision tree with different compression rates. The results where it utilizes the Naive Bayes classifier and a higher compression rate beeing more yielding better precision and recall.

Creating a Characterization is quite similar to making a Summarization of character related content but could also include deductions made from the behavior of that character.
A recent Paper from 2021 \cite{brahman-etal-2021-characters-tell} presents a dataset called LiSCU (Literary Summaries with Character Understanding) that aims to facilitate research in character-centric narrative understanding. They used techniques for Character Identification, where the goal is to identify a character's name from an anonymized description, and Character Description Generation, which involves generating a description for a given character based on a literature summary. 

might exceed model limits:
Length Truncation: Simply truncating the summary at the end.
Coreference Truncation: Using SpanBERT to identify sentences in the summary that mention the character, focusing on these sentences.



GPT-2: With a maximum input length of 1024 tokens.
BART (Bidirectional and Auto-Regressive Transformers): Extended to accept up to 2048 tokens.


Longformer: Leveraged for its efficient encoding mechanism to handle long texts, allowing inputs up to 16,384 tokens when using the full text of books.

BLEU-4, ROUGE-n (n=1, 2), ROUGE-L F-1 scores, and BERTScore to measure similarity and quality.


performed better with length truncation

Errors in coreference resolution impacted the coreference truncation performance.



\cite*{schroder-etal-2021-neural}
coarse-to-fine approach, which first generates coarse coreference clusters and then refines them. This method allows the model to handle the complexity of coreference resolution by breaking it down into more manageable steps.
Two primary neural network models were developed: the base model and the large model. The large model uses the ELECTRA-large model for contextual embeddings, while the base model uses the ELECTRA-base model.
Data Preprocessing

% The models were trained on multiple datasets, including SemEval-2010, TüBa-D\\/Z, OntoNotes 5.0, and the DROC dataset. These datasets provide a diverse range of documents, which helps in training robust coreference resolution models.
Special attention was given to handling singletons, which are mentions that do not corefer with any other mention in the document. A discard functionality was introduced to manage singletons effectively.
Training and Evaluation:

The models were trained using a variety of loss functions and optimization techniques to ensure convergence and high performance.
The performance was measured using the CoNLL-F1 score, which is a standard metric for coreference resolution tasks.
Results
Performance

% The coarse-to-fine models significantly outperformed previous state-of-the-art systems on both the SemEval-2010 and TüBa-D/Z datasets. The improvements were substantial, with the model achieving an increase of +25.85 F1 on SemEval-2010 and +30.25 F1 on TüBa-D\\/Z.
% Even when compared to systems using gold mentions, which are mentions manually annotated in the dataset, the models still showed a performance increase of more than 10 F1 points.
% Impact of Model Variations

% The use of the ELECTRA-large model for contextual embeddings provided a small but notable improvement over the base model, with an increase of +1.58 F1 on TüBa-D\\/Z and +1.92 F1 on SemEval-2010.
% Different configurations and model variations were tested to analyze their impact on performance. It was found that models including a discard functionality for singletons performed better.
% Error Analysis

The error analysis indicated that the coarse-to-fine model generally produced accurate coreference links both locally and document-wide. However, there were frequent errors related to missed and added mentions. These errors were attributed to inconsistent training signals and the inherent complexity of coreference tasks.
The analysis also highlighted that the model’s performance decreases as the document length increases, which aligns with previous findings in coreference resolution research.
Visualizations and Examples:

The paper includes visualizations and specific examples to demonstrate the model’s predictions on unseen documents. These examples show how the model accurately predicts coreference relationships in complex sentences, validating its effectiveness in practical scenarios.
Overall, the methods and results presented in the paper highlight the significant advancements made in coreference resolution through the use of coarse-to-fine neural network models. The study provides a comprehensive evaluation of these models, demonstrating their superiority over existing systems .

\subsection{Project Gutenberg}
Project Gutenberg, founded in 1971 by Michael S. Hart, is one of the oldest and most extensive digital libraries, aimed at providing free access to a vast collection of over 60,000 eBooks. Hart's initiative began with the digitization of the United States Declaration of Independence, setting the stage for the project's goal of democratizing access to literature and cultural works. Named after Johannes Gutenberg, the inventor of the printing press, Project Gutenberg echoes his mission of making written works widely accessible. The Project Gutenberg Literary Archive Foundation, a non-profit organization, oversees the project's administration, legal issues, and fundraising efforts.

\section{RAG}
In contrast to their approach for Character Description Generation which required modeling long-range dependencies, I am using Retrieval-augmented generation (RAG), which is a technique to improve the quality of LLM-generated responses by grounding the model on external sources. LLMs are inconsistent in terms of producing same quality responses for each and every topic, since they knowledge is based on finite amount of information, that isn't equally distributed for every potential topic. But Retrieval-augmented generation doesn't only reduce the need for internal sources (continuous training, lowering computational and financial costs) but also ensures that the model has access to the most current, reliable facts.
In this thesis I am primarily focusing on getting those important properties and behavior (key features) from the characters described in the literature to achieve better characterizations with grounded models that utilize this external information.


\section{Query generation}
There are multiple approaches to consider when selecting the optimal prompts and information for a language model to generate high-quality summaries or characterizations. While rephrasing a task for a language model can influence the prompt's outcome to some degree, this study will primarily focus on selecting and curating the relevant literature data, which will be appended to the task of the prompt.\\

One method to obtain the necessary information is to filter the text for sentences containing certain keywords. However, simply finding all sentences that mention a character's name is insufficient for a comprehensive description. Critical information about the character may be present in sentences that do not explicitly mention their name but refer to them indirectly. Consequently, important details can be missed using this technique and also additionally, this approach might include too much unnecessary information, especially for main characters, making it unsuitable for zero-shot character summary generation.\\

(https://huggingface.co/intfloat/e5-mistral-7b-instruct) expand...
To improve upon this, text embeddings can be utilized. For instance, using a model like E5-Mistral-7B-Instruct, which has 32 layers and an embedding size of 4096, we can chunk the literature into sections of roughly equal length and embed each chunk. This allows us to identify chunks that satisfy a certain premise, such as describing a particular character more accurately than others.\\

Further improvements can be achieved by applying coreference resolution techniques (\cite{schroder-etal-2021-neural}, \cite{dobrovolskii-2021-word}) to identify all tokens that refer to the given entity. This helps in gathering more sentences relevant to the characters context.\\

If it is possible to identify self-contained content scopes using coreference resolution and segmenting the content by highly self-referenced text passages, the language model can generate even better character profiles due to the additional relevant information.\\



Another approach to consider is fine-tuning a language model like LLAMA to enhance text summarization results.\

The generated characterizations can be evaluated both qualitatively and quantitatively. To compare human-written characterizations with those generated by the model, we can measure recall and precision using metrics like ROUGE and BLEU, and employ BERTScore as a semantic evaluation metric.\\

Since language models are typically trained on extensive data, they might already contain information about certain books. To test this, we can compare queries that include key sentences to those that omit them. If the model produces the same output despite the missing key information, it suggests prior training on that data. Additionally, using books released after the model's training period ensures no pre-existing knowledge about the characters.\\

Existing human-written characterizations will serve as benchmarks for assessing the model's output in terms of style, content, structure, and level of detail.